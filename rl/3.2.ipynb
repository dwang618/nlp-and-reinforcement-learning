{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "num_of_states = 3\n",
    "gamma = 0.9\n",
    "\n",
    "def create_distribution(n, temp=1.0):\n",
    "    logits = np.random.randn(n)\n",
    "    exp_logits = np.exp(logits / temp)\n",
    "    prob = exp_logits / exp_logits.sum() \n",
    "    return prob\n",
    "\n",
    "\n",
    "def create_MRP():\n",
    "    P = np.zeros((num_of_states, num_of_states))\n",
    "    for s in range(num_of_states):\n",
    "        P[s, :] = create_distribution(num_of_states)\n",
    "    r = np.random.randn(num_of_states, 1)\n",
    "    return P, r\n",
    "\n",
    "def policy_evaluation(P, r, gamma):\n",
    "    bellman_operator = lambda v: r + gamma * P @ v\n",
    "    v = np.random.randn(P.shape[0], 1)  # Initialize value function\n",
    "    error = float('inf')\n",
    "    while error > 1e-5:\n",
    "        v_prev = v\n",
    "        # Update value function for state s\n",
    "        v = bellman_operator(v_prev)\n",
    "        error = np.mean(np.abs(v-v_prev))\n",
    "        #print(error)\n",
    "    return v\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_of_actions = 2\n",
    "\n",
    "def create_MDP():\n",
    "    P = np.zeros((num_of_states, num_of_actions, num_of_states))\n",
    "    for s in range(num_of_states):\n",
    "        for a in range(num_of_actions):\n",
    "            P[s, a, :] = create_distribution(num_of_states)\n",
    "    r = np.random.randn(num_of_states, num_of_actions)\n",
    "    return P, r\n",
    "\n",
    "def initial_policy():\n",
    "    pi = np.zeros((num_of_states, num_of_actions))\n",
    "    for s in range(num_of_states):\n",
    "        pi[s, :] = create_distribution(num_of_actions)\n",
    "    return pi\n",
    "\n",
    "def policy_iteration(P, r, gamma, tol=1e-5):\n",
    "    pi = initial_policy()  # Initialize policy\n",
    "    r = np.reshape(r, (num_of_states * num_of_actions, 1))\n",
    "    q = np.zeros((num_of_states, num_of_actions))  # Initialize Q-values\n",
    "    q_prev = np.copy(q)  # Previous Q-values for convergence checking\n",
    "\n",
    "    while True:\n",
    "        # next 1: Policy Evaluation\n",
    "        P_pi = np.zeros((num_of_states, num_of_actions, num_of_states, num_of_actions))\n",
    "        \n",
    "        # next 2: Policy Improvement\n",
    "        for s in range(num_of_states):\n",
    "            for a in range(num_of_actions):\n",
    "                for s_prime in range(num_of_states):\n",
    "                    for a_prime in range(num_of_actions):\n",
    "                        P_pi[s, a, s_prime, a_prime] = P[s, a, s_prime] * pi[s_prime, a_prime]\n",
    "        \n",
    "        P_pi = np.reshape(P_pi, (num_of_states * num_of_actions, num_of_states * num_of_actions))\n",
    "        q = policy_evaluation(P_pi, r, gamma)\n",
    "        print(q)\n",
    "        q = np.reshape(q, (num_of_states, num_of_actions))\n",
    "        \n",
    "        # Check for convergence: if the difference between q and q_prev is less than tolerance\n",
    "        if np.max(np.abs(q - q_prev)) < tol:\n",
    "            break  # Convergence achieved\n",
    "        \n",
    "        q_prev = np.copy(q)\n",
    "        \n",
    "        # Policy improvement: Update policy based on the new Q-values\n",
    "        pi = np.zeros((num_of_states, num_of_actions))\n",
    "        for s in range(num_of_states):\n",
    "            pi[s, np.argmax(q[s, :])] = 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[-2.69117907]\n",
      " [-1.25173211]\n",
      " [-1.94673296]\n",
      " [-2.08200641]\n",
      " [-2.4079183 ]\n",
      " [-4.29199762]]\n",
      "[[3.71861577]\n",
      " [5.10363459]\n",
      " [4.76977167]\n",
      " [4.55449481]\n",
      " [4.03144054]\n",
      " [2.28712057]]\n",
      "[[3.71861358]\n",
      " [5.10363239]\n",
      " [4.76976948]\n",
      " [4.55449262]\n",
      " [4.03143835]\n",
      " [2.28711838]]\n"
     ]
    }
   ],
   "source": [
    "P, r = create_MDP()\n",
    "policy_iteration(P, r, gamma)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
